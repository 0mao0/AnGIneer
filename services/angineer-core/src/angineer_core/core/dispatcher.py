"""
æ‰§è¡Œè°ƒåº¦æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ SOP æ­¥éª¤ç¼–æ’ã€å·¥å…·è°ƒç”¨ä¸ä¸Šä¸‹æ–‡æ›´æ–°ã€‚

æ”¯æŒä¾èµ–æ³¨å…¥ï¼š
- memory: Memory å®ä¾‹
- llm_client: LLMClient å®ä¾‹
"""
import time
import json
import os
from typing import Dict, Any, Tuple, List, Optional, TYPE_CHECKING
from angineer_core.standard.context_models import SOP, Step
from angineer_core.core.memory import Memory, StepRecord
from angineer_core.infra.logger import get_logger

logger = get_logger(__name__)

if TYPE_CHECKING:
    from angineer_core.infra.llm_client import LLMClient

try:
    from engtools.BaseTool import ToolRegistry
except ImportError:
    ToolRegistry = None

from angineer_core.infra.llm_client import llm_client as default_llm_client


class Dispatcher:
    def __init__(
        self,
        config_name: str = None,
        mode: str = "instruct",
        result_md_path: str = None,
        memory: Optional[Memory] = None,
        llm_client: Optional["LLMClient"] = None
    ):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨ä¸Šä¸‹æ–‡ä¸æ¨¡å‹é…ç½®ã€‚
        
        Args:
            config_name: LLM é…ç½®åç§°
            mode: æ‰§è¡Œæ¨¡å¼
            result_md_path: Markdown æ—¥å¿—æ–‡ä»¶è·¯å¾„
            memory: å¯é€‰çš„ Memory å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
            llm_client: å¯é€‰çš„ LLMClient å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.memory = memory or Memory()
        self.config_name = config_name
        self.mode = mode or "instruct"
        self.result_md_path = result_md_path
        self._llm_client = llm_client or default_llm_client
        self.variable_metadata = {}
        self.start_time = None
        self.step_durations = {}
        self.summary_durations = {}
        self.tool_durations = {}
        
        if self.result_md_path:
            with open(self.result_md_path, "w", encoding="utf-8") as f:
                f.write("# SOP æ‰§è¡Œæ—¥å¿— (LLM é£æ ¼å°ç»“ç‰ˆ)\n\n")
                f.write("> **è¯´æ˜**: æœ¬æ—¥å¿—å±•ç¤ºäº†æ¯ä¸€æ­¥çš„æ‰§è¡Œå°ç»“ä¸ Blackboard çŠ¶æ€å¿«ç…§ã€‚æ›´æ–°çš„å†…å®¹å·²é«˜äº®æ˜¾ç¤ºã€‚\n\n")
    
    @property
    def llm_client(self):
        """è·å– LLM å®¢æˆ·ç«¯ã€‚"""
        return self._llm_client
    
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """
        ä» LLM å“åº”ä¸­æå– JSON æ•°æ®ã€‚
        
        æ”¯æŒä»¥ä¸‹æ ¼å¼:
        - ```json {...} ```
        - ``` {...} ```
        - çº¯ JSON å­—ç¬¦ä¸²
        
        Args:
            response: LLM çš„åŸå§‹å“åº”å­—ç¬¦ä¸²
            
        Returns:
            è§£æåçš„ JSON å­—å…¸
            
        Raises:
            json.JSONDecodeError: å½“æ— æ³•è§£æ JSON æ—¶
        """
        cleaned = response.strip()
        
        if "```json" in cleaned:
            cleaned = cleaned.split("```json")[1].split("```")[0]
        elif "```" in cleaned:
            cleaned = cleaned.split("```")[1].split("```")[0]
        
        return json.loads(cleaned.strip())
    
    def log_pre_execution(self, logs: List[Dict[str, Any]]):
        """
        è®°å½•å‰ç½®è¿‡ç¨‹æ—¥å¿—åˆ° Markdownã€‚
        logs: list of dict, each containing:
            - event: äº‹ä»¶åç§° (e.g. "ç”¨æˆ·éœ€æ±‚")
            - method: è·å¾—æ–¹å¼ (e.g. "User Input")
            - time: å‘ç”Ÿæ—¶é—´ (e.g. "2023-10-27 10:00:00")
            - duration: è€—æ—¶ (e.g. "0.5s")
            - details: è¯¦ç»†å†…å®¹
        """
        if not self.result_md_path:
            return
            
        with open(self.result_md_path, "a", encoding="utf-8") as f:
            f.write("## 0. å‰ç½®è¿‡ç¨‹æ¦‚è§ˆ\n\n")
            f.write("| äº‹ä»¶ | è·å¾—æ–¹å¼ | æ—¶é—´ | è€—æ—¶ | è¯¦æƒ… |\n")
            f.write("| --- | --- | --- | --- | --- |\n")
            
            for log in logs:
                event = log.get("event", "-")
                method = log.get("method", "-")
                time_str = log.get("time", "-")
                duration = log.get("duration", "-")
                details = str(log.get("details", "-")).replace("\n", "<br>")
                if len(details) > 100:
                    details = details[:97] + "..."
                    
                f.write(f"| {event} | {method} | {time_str} | {duration} | {details} |\n")
            
            f.write("\n---\n\n")

    def run(self, sop: SOP, initial_context: Dict[str, Any], pre_logs: List[Dict[str, Any]] = None):
        """
        Execute the SOP with the given initial context.
        """
        self.start_time = time.time()
        logger.info(f"[{sop.id}] Starting execution: {sop.description}")
        
        # Log pre-execution events if provided
        if pre_logs:
            self.log_pre_execution(pre_logs)
            
        self.memory.update_context(initial_context)
        
        # Simple linear execution for now
        # In a real FSM, we would follow next_step_id
        for step in sop.steps:
            self._execute_step(step)
            
        logger.info(f"[{sop.id}] Execution finished.")
        
        # Log total time
        total_duration = time.time() - self.start_time
        
        # Calculate breakdowns
        total_tool_time = sum(self.tool_durations.values())
        total_summary_time = sum(self.summary_durations.values())
        total_step_overhead = sum(self.step_durations.values()) - total_tool_time - total_summary_time
        
        if self.result_md_path:
            with open(self.result_md_path, "a", encoding="utf-8") as f:
                f.write(f"## æ‰§è¡Œæ€»ç»“\n\n")
                f.write(f"| é¡¹ç›® | è€—æ—¶ | å æ¯” |\n")
                f.write(f"| --- | --- | --- |\n")
                f.write(f"| **æ€»è€—æ—¶** | **{total_duration:.2f}s** | 100% |\n")
                f.write(f"| å·¥å…·æ‰§è¡Œ | {total_tool_time:.2f}s | {(total_tool_time/total_duration)*100:.1f}% |\n")
                f.write(f"| LLM æ€»ç»“ | {total_summary_time:.2f}s | {(total_summary_time/total_duration)*100:.1f}% |\n")
                f.write(f"| è°ƒåº¦å¼€é”€ | {total_step_overhead:.2f}s | {(total_step_overhead/total_duration)*100:.1f}% |\n")
                f.write(f"\n> æ³¨: 'è°ƒåº¦å¼€é”€' åŒ…å« Python ä»£ç æ‰§è¡Œã€æ–‡ä»¶ I/O åŠå…¶ä»–é€»è¾‘å¤„ç†æ—¶é—´ã€‚\n")
        
        return self.memory.blackboard
        
    def _execute_step(self, step: Step):
        step_start = time.time()
        logger.info(f"Executing Step: {step.name or step.id} ({step.tool})")
        
        # [Hybrid Architecture Check]
        # If this step was generated by LLM analysis (Scenario A)
        if getattr(step, "analysis_status", None) == "analyzed":
            self._execute_analyzed_step(step)
        else:
            # [Classic Logic] (Scenario B)
            # 1. Resolve Inputs
            tool_inputs = {}
            for key, value in step.inputs.items():
                resolved_value = self.memory.resolve_value(value)
                tool_inputs[key] = resolved_value
                
            # 2. Determine Tool (Static or Auto)
            target_tool_name = step.tool
            if target_tool_name == "auto":
                logger.debug("Detecting tool via LLM...")
                detected_tool, detected_inputs = self._smart_select_tool(step, tool_inputs)
                if detected_tool:
                    logger.info(f"Auto selected tool: {detected_tool}")
                    target_tool_name = detected_tool
                    tool_inputs.update(detected_inputs)
                else:
                    logger.warning("Auto tool selection failed")
                    self._record_step(step, tool_inputs, None, error="Auto-selection failed")
                    return
    
            # 3. Execute Tool
            self._execute_tool_safe(target_tool_name, tool_inputs, step)
            
        # Record duration
        duration = time.time() - step_start
        self.step_durations[step.id] = duration
        
        # If markdown log was written inside _execute_tool_safe, we need to inject duration there?
        # Actually _execute_tool_safe calls _write_markdown_log.
        # But at that point we don't have the full duration (summary generation takes time too).
        # We might need to pass start time to _write_markdown_log or update it later.
        # Simpler approach: Calculate tool execution time inside _execute_tool_safe and pass it.

    def _execute_analyzed_step(self, step: Step):
        """
        Execute a step that was analyzed by LLM (Hybrid Mode).
        Logic:
        1. Resolve all inputs from context.
        2. Check if any resolved input indicates missing data (e.g. explicit None or unresolved vars).
        3. Check if 'notes' exist.
        4. If (Missing Inputs OR Notes OR Tool='auto'), wake up LLM.
        5. Else, execute directly.
        """
        context = self.memory.blackboard
        # Check if step outputs already exist in context
        if self._should_skip_step(step, context):
            self._record_step(step, {}, {"skipped": True, "reason": "value exists in context"})
            return
            
        missing_params = []
        ready_inputs = {}
        
        # 1. Resolve Inputs
        for param_name, value_expr in step.inputs.items():
            resolved_value = self.memory.resolve_value(value_expr)
            ready_inputs[param_name] = resolved_value
            
            # Simple check: if resolved value looks like an unresolved template or None
            if resolved_value is None:
                 missing_params.append(f"{param_name} (value is None)")
            elif isinstance(resolved_value, str) and "${" in resolved_value:
                 # Check if it's an unresolved reference
                 # This is a heuristic, but often useful
                 missing_params.append(f"{param_name} (unresolved: {resolved_value})")

        # 2. Decision Logic
        needs_llm = False
        reason = ""
        
        if missing_params:
            needs_llm = True
            reason = f"Missing parameters: {missing_params}"
        elif step.notes:
            needs_llm = True
            reason = f"Notes present: {step.notes}"
        elif step.tool == "auto":
            needs_llm = True
            reason = "Tool is 'auto'"
            
        if needs_llm:
            logger.debug(f"Hybrid mode: waking up LLM - {reason}")
            self._smart_step_execution(step, reason, missing_params)
        else:
            logger.debug("Hybrid mode: rule-based execution (all params ready)")
            # Execute directly
            self._execute_tool_safe(step.tool, ready_inputs, step)

    def _should_skip_step(self, step: Step, context: Dict[str, Any]) -> bool:
        """Check if step outputs are already present in context."""
        if not step.outputs:
            return False
        # If outputs is "*" we can't easily check, so don't skip
        if step.outputs == "*":
            return False
            
        output_keys = list(step.outputs.keys())
        if not output_keys:
            return False
            
        # Check if all output keys exist and are not None
        # Exception: if the value in context is explicitly "None" string or something? No.
        return all(key in context and context.get(key) is not None for key in output_keys)

    def _execute_tool_safe(self, tool_name: str, inputs: Dict[str, Any], step: Step):
        """Helper to execute tool and record history"""
        if ToolRegistry is None:
            error_msg = "ToolRegistry not available (engtools not installed)"
            logger.error(error_msg)
            self._record_step(step, inputs, None, error=error_msg)
            raise RuntimeError(error_msg)
            
        tool = ToolRegistry.get_tool(tool_name)
        if not tool:
            error_msg = f"Tool not found: {tool_name}"
            logger.error(error_msg)
            self._record_step(step, inputs, None, error=error_msg)
            raise RuntimeError(error_msg)
            
        try:
            run_kwargs = dict(inputs)
            if self.config_name:
                run_kwargs["config_name"] = self.config_name
            if self.mode:
                run_kwargs["mode"] = self.mode
                
            tool_start = time.time()
            result = tool.run(**run_kwargs)
            tool_duration = time.time() - tool_start
            
            # Record tool duration
            self.tool_durations[step.id] = tool_duration
            
            logger.debug(f"Tool result: {result}")
            
            # Process outputs using the standard method
            updates = self._process_outputs(step, result)
            
            # Record history
            self._record_step(step, inputs, result)
            
            # Write log
            if self.result_md_path:
                self._write_markdown_log(step, inputs, result, updates, duration=tool_duration)
                
        except Exception as e:
            logger.error(f"Tool execution error: {e}")
            self.tool_durations[step.id] = 0.0
            self._record_step(step, inputs, None, error=str(e))
            if self.result_md_path:
                 self._write_markdown_log(step, inputs, {"error": str(e)}, {}, duration=0.0)

    def _handle_action_return_value(self, step: Step, action_data: Dict[str, Any]):
        """å¤„ç† return_value Actionï¼šç›´æ¥è¿”å›å€¼ã€‚"""
        value = action_data.get("value")
        logger.info(f"Returning value: {value}")
        updates = self._process_outputs(step, value)
        self._record_step(step, {}, value)
        if self.result_md_path:
            self._write_markdown_log(step, {}, value, updates)

    def _handle_action_ask_user(self, step: Step, action_data: Dict[str, Any]):
        """å¤„ç† ask_user Actionï¼šå‘ç”¨æˆ·è¯¢é—®è¾“å…¥ã€‚"""
        question = action_data.get("question")
        variable = action_data.get("variable")
        logger.info(f"Asking user: {question}")
        inputs = {"question": question}
        if variable:
            inputs["variable"] = variable
        self._execute_tool_safe("user_input", inputs, step)

    def _handle_action_execute_tool(self, step: Step, action_data: Dict[str, Any]):
        """å¤„ç† execute_tool Actionï¼šæ‰§è¡ŒæŒ‡å®šå·¥å…·ã€‚"""
        tool_name = action_data.get("tool")
        inputs = action_data.get("inputs", {})
        self._execute_tool_safe(tool_name, inputs, step)

    def _handle_action_skip(self, step: Step, action_data: Dict[str, Any]):
        """å¤„ç† skip Actionï¼šè·³è¿‡æ­¥éª¤ã€‚"""
        skip_reason = action_data.get('reason', 'No reason provided')
        logger.info(f"Skipping step: {skip_reason}")
        self._record_step(step, {}, {"skipped": True, "reason": skip_reason})
        if self.result_md_path:
            self._write_markdown_log(step, {}, {"skipped": True, "reason": skip_reason}, {})

    def _handle_action_unknown(self, step: Step, action: str):
        """å¤„ç†æœªçŸ¥çš„ Action ç±»å‹ã€‚"""
        error_msg = f"Unknown action: {action}"
        logger.error(error_msg)
        self._record_step(step, {}, None, error=error_msg)

    def _smart_step_execution(self, step: Step, reason: str, missing_params: List[str]):
        """
        LLM æ™ºèƒ½æ‰§è¡Œæ­¥éª¤ã€‚

        é€šè¿‡ LLM åˆ†æå½“å‰æ­¥éª¤çŠ¶æ€ï¼Œå†³å®šæ‰§è¡Œä½•ç§æ“ä½œæ¥å®Œæˆæ­¥éª¤ã€‚
        æ”¯æŒçš„æ“ä½œåŒ…æ‹¬ï¼šè¯¢é—®ç”¨æˆ·ã€æ‰§è¡Œå·¥å…·ã€è¿”å›å€¼ã€è·³è¿‡ã€‚
        æ³¨æ„ï¼šå…·ä½“çš„å·¥å…·æ‰§è¡Œç”± LLM è¿”å› execute_tool actionï¼Œé€šè¿‡ tool å­—æ®µæŒ‡å®šå·¥å…·åã€‚
        
        Args:
            step: å½“å‰æ‰§è¡Œçš„æ­¥éª¤
            reason: éœ€è¦ LLM ä»‹å…¥çš„åŸå› 
            missing_params: ç¼ºå¤±çš„å‚æ•°åˆ—è¡¨
        """
        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        context_str = json.dumps(self.memory.get_context_snapshot(), default=str, ensure_ascii=False)
        if len(context_str) > 3000:
            context_str = context_str[:3000] + "..."
        
        # æ„å»º Prompt å¹¶è°ƒç”¨ LLM
        system_prompt = self._build_smart_execution_prompt(step, reason, context_str)
        messages = [{"role": "system", "content": system_prompt}]
        
        try:
            response = self.llm_client.chat(messages, mode=self.mode, config_name=self.config_name)
            action_data = self._extract_json_from_response(response)
            action = action_data.get("action")
            
            logger.debug(f"AI decision: {action}")
            
            # ä½¿ç”¨ç­–ç•¥æ¨¡å¼åˆ†å‘åˆ°å¯¹åº”çš„å¤„ç†æ–¹æ³•
            action_handlers = {
                "return_value": self._handle_action_return_value,
                "ask_user": self._handle_action_ask_user,
                "execute_tool": self._handle_action_execute_tool,
                "skip": self._handle_action_skip,
            }
            
            handler = action_handlers.get(action)
            if handler:
                handler(step, action_data)
            else:
                self._handle_action_unknown(step, action)
                
        except Exception as e:
            error_msg = f"Smart execution error: {e}"
            logger.error(error_msg)
            self._record_step(step, {}, None, error=error_msg)

    def _write_markdown_log(self, step: Step, inputs: Any, result: Any, updates: Dict[str, Any], duration: float = 0.0):
        """Write step execution details to Markdown file"""
        if not self.result_md_path:
            return
            
        blackboard_values = self.memory.blackboard
        step_id = step.id
        step_name = step.name or step_id
        tool_name = step.tool
        description = step.description or ""
        
        # Determine current step note based on tool
        current_step_note = f"å·¥å…·: {tool_name}"
        if tool_name == "table_lookup":
             table_name = inputs.get('table_name', '') if isinstance(inputs, dict) else ''
             current_step_note = f"æŸ¥è¡¨: {table_name}"
        elif tool_name == "calculator":
             expr = inputs.get('expression', '') if isinstance(inputs, dict) else ''
             if len(expr) > 25:
                 expr = expr[:22] + "..."
             current_step_note = f"å…¬å¼: {expr}" if expr else "å…¬å¼è®¡ç®—"
        elif tool_name == "user_input":
             current_step_note = "ç”¨æˆ·è¾“å…¥"
        elif tool_name == "auto":
             current_step_note = "è‡ªåŠ¨ç”Ÿæˆ"
             
        # Update metadata for new variables
        for key in updates:
            self.variable_metadata[key] = {
                "source_step": step_id,
                "note": current_step_note,
                "duration": duration
            }
        
        with open(self.result_md_path, "a", encoding="utf-8") as f:
            f.write(f"## {step_id}: {step_name}\n\n")
            
            # 1. å†™å…¥ LLM å°ç»“
            summary_start = time.time()
            llm_summary = self._generate_step_summary(step_name, tool_name, inputs, result, updates)
            summary_duration = time.time() - summary_start
            self.summary_durations[step.id] = summary_duration
            
            f.write(f"**LLM å°ç»“** (è€—æ—¶: {summary_duration:.2f}s): {llm_summary}\n\n")
            
            # 2. å†™å…¥ Blackboard æ›´æ–°è¡¨æ ¼
            f.write(f"**Blackboard çŠ¶æ€**:\n\n")
            f.write("| åºå· | å‚æ•° | ç±»å‹ | å–å€¼ | çŠ¶æ€ | è€—æ—¶ | å¤‡æ³¨ |\n")
            f.write("| --- | --- | --- | --- | --- | --- | --- |\n")
            
            # å›ºå®šé¡ºåºï¼šæŒ‰å­—æ¯åºæ’åº
            all_keys = sorted(blackboard_values.keys())
            
            for idx, key in enumerate(all_keys, 1):
                val = blackboard_values.get(key)
                
                # Default values
                status = "âšª å·²çŸ¥é‡"
                note = "-"
                time_str = "-"
                
                if key in updates:
                    status = f"ğŸŸ¢ {step_id} ç»“æœ"
                    note = current_step_note
                    time_str = f"{duration:.2f}s"
                elif key in self.variable_metadata:
                    meta = self.variable_metadata[key]
                    source = meta.get("source_step", "Unknown")
                    status = f"ğŸŸ¡ {source} æ±‚è§£"
                    note = meta.get("note", "-")
                    prev_duration = meta.get("duration", 0.0)
                    time_str = f"{prev_duration:.2f}s" if prev_duration > 0 else "-"
                else:
                    status = "âšª å·²çŸ¥é‡"
                    note = "åˆå§‹å‚æ•°"
                    time_str = "-"

                # Type Inference (Simple)
                val_type = type(val).__name__
                if isinstance(val, (int, float)):
                    val_type = "æ•°å€¼"
                elif isinstance(val, str):
                    val_type = "å­—ç¬¦ä¸²"
                
                # Format Value (Truncate if too long)
                val_str = str(val)
                # Escape pipe characters to avoid breaking the table
                val_str = val_str.replace("|", "\\|").replace("\n", " ")
                if len(val_str) > 50:
                    val_str = val_str[:47] + "..."

                f.write(f"| {idx} | {key} | {val_type} | {val_str} | {status} | {time_str} | {note} |\n")
                
            f.write("\n")
            
            # 3. è¯¦ç»†å·¥å…·æ—¥å¿—ï¼ˆæŠ˜å ï¼‰
            f.write("<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹å·¥å…·è°ƒç”¨è¯¦æƒ…</summary>\n\n")
            f.write(f"**è¯´æ˜**: {description}\n\n")
            f.write(f"**å·¥å…·**: `{tool_name}`\n\n")
            f.write(f"**è€—æ—¶**: {duration:.4f}s\n\n")
            f.write("**è¾“å…¥**:\n")
            f.write(f"```json\n{json.dumps(inputs, ensure_ascii=False, indent=2)}\n```\n\n")
            f.write("**è¾“å‡º**:\n")
            f.write(f"```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```\n\n")
            f.write("</details>\n\n")
            f.write("---\n\n")

    def _process_outputs(self, step: Step, result: Any) -> Dict[str, Any]:
        # Update global context based on output mapping
        updates = {}
        if not step.outputs:
            return updates
            
        # If outputs is "*" map everything (if result is dict)
        if step.outputs == "*":
            if isinstance(result, dict):
                updates = result
                self.memory.update_context(result)
            else:
                updates = {"last_result": result}
                self.memory.update_context(updates)
            return updates
            
        for context_key, result_path in step.outputs.items():
            # Simple extraction
            # If result_path is empty string or ".", use the whole result
            if not result_path or result_path == ".":
                val = result
            # If result_path is "result", extract the 'result' field from dict
            elif result_path == "result":
                if isinstance(result, dict) and "result" in result:
                    val = result["result"]
                else:
                    val = result  # Fallback to whole result if no 'result' field
            elif isinstance(result, dict) and result_path in result:
                val = result[result_path]
            else:
                # Try to treat result_path as a literal constant (e.g. "0.15", "-1", "True")
                try:
                    # Check for boolean first
                    if result_path.lower() == "true":
                         val = True
                    elif result_path.lower() == "false":
                         val = False
                    else:
                         # Try float/int
                         # Remove whitespace
                         rp = result_path.strip()
                         val = float(rp)
                         # Convert to int if it's an integer value and original string didn't look like a float (optional)
                         if val.is_integer() and '.' not in rp:
                             val = int(val)
                except:
                     val = None
                
            if val is not None:
                updates[context_key] = val
                self.memory.update_context({context_key: val})
        
        return updates
                
    def _record_step(self, step: Step, inputs: Any, outputs: Any, error: str = None):
        record = StepRecord(
            step_id=step.id,
            tool_name=step.tool,
            inputs=inputs,
            outputs=outputs,
            status="failed" if error else "success",
            error=error
        )
        self.memory.add_step_io({
            "step_id": step.id,
            "tool_name": step.tool,
            "inputs": inputs,
            "outputs": outputs,
            "status": "failed" if error else "success",
            "error": error
        })
        self.memory.add_history(record)



    def _smart_select_tool(self, step: Step, current_inputs: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        """
        Use LLM to select the best tool and formulate inputs when step.tool is 'auto'.
        """
        if ToolRegistry is None:
            return None, {}
            
        tools_desc = ToolRegistry.list_tools()
        tools_str = "\n".join([f"- {name}: {desc}" for name, desc in tools_desc.items()])
        
        # Prepare context snapshot (truncated to avoid huge prompt)
        context_str = json.dumps(self.memory.get_context_snapshot(), default=str, ensure_ascii=False)
        if len(context_str) > 2000:
            context_str = context_str[:2000] + "...(truncated)"
            
        system_prompt = f"""
You are an intelligent agent dispatcher. Your task is to select the most appropriate tool to execute the current step.

Available Tools:
{tools_str}

Current Step Information:
- ID: {step.id}
- Description: {step.description_zh or step.description}
- Pre-resolved Inputs: {json.dumps(current_inputs, default=str, ensure_ascii=False)}

Global Context:
{context_str}

Instructions:
1. Analyze the step description and context.
2. Select the best tool from the available list to accomplish the step goal.
3. Extract or formulate the necessary arguments for the tool based on context and inputs.
4. Return a JSON object with "tool" and "inputs".

Example Output:
{{
  "tool": "calculator",
  "inputs": {{ "expression": "12 * 50" }}
}}
"""
        messages = [{"role": "system", "content": system_prompt}]
        try:
            response = self.llm_client.chat(messages, mode=self.mode, config_name=self.config_name)
            data = self._extract_json_from_response(response)
            return data.get("tool"), data.get("inputs", {})
        except Exception as e:
            logger.error(f"Smart selection failed: {e}")
            return None, {}

            
    def _build_smart_execution_prompt(self, step: Step, reason: str, context_str: str) -> str:
        """
        æ„å»ºæ™ºèƒ½æ‰§è¡Œæ­¥éª¤çš„ System Promptã€‚
        
        Args:
            step: å½“å‰æ‰§è¡Œçš„æ­¥éª¤
            reason: éœ€è¦ LLM ä»‹å…¥çš„åŸå› 
            context_str: ä¸Šä¸‹æ–‡å˜é‡å­—ç¬¦ä¸²
            
        Returns:
            æ„å»ºå¥½çš„ system prompt
        """
        return f"""You are the Step Executor of an expert system. You are facing a step that requires your attention.

Current Step:
- Name: {step.name}
- Description: {step.description}
- Notes/Warnings: {step.notes}
- Required Inputs: {step.inputs}

Context Variables:
{context_str}

Situation: {reason}

Your Goal: Complete this step or make progress towards it.

Available Actions (Output JSON):
1. ASK_USER: If parameters are missing and you cannot deduce them.
   {{ "action": "ask_user", "question": "...", "variable": "å˜é‡å" }}
   
2. SEARCH_KNOWLEDGE: If you need to check textual regulations.
    {{ "action": "search_knowledge", "query": "..." }}
    
 3. TABLE_LOOKUP: If you need to find a value in a standard table (e.g. Dimensions of 10000 DWT ship).
    {{ "action": "table_lookup", "table_name": "...", "conditions": "...", "target_column": "..." }}

 4. EXECUTE_TOOL: If you have enough info to run the tool (calculator, etc).
    {{ "action": "execute_tool", "tool": "{step.tool if step.tool != 'auto' else 'appropriate_tool'}", "inputs": {{ ... }} }}
    
 5. RETURN_VALUE: If the step is just setting a value or you know the answer directly.
    {{ "action": "return_value", "value": 0.15 }}

 6. SKIP: If this step is already done or irrelevant.
    {{ "action": "skip", "reason": "..." }}

Output ONLY the JSON."""

    def _generate_step_summary(self, step_name: str, tool_name: str, resolved_inputs: Any, result: Any, updates: Dict[str, Any]) -> str:
        """Use LLM to generate a natural language summary of the step execution."""
        try:
            # Prepare data for prompt, truncating large structures
            inputs_str = json.dumps(resolved_inputs, default=str, ensure_ascii=False)
            if len(inputs_str) > 500: inputs_str = inputs_str[:500] + "..."
            
            result_str = json.dumps(result, default=str, ensure_ascii=False)
            if len(result_str) > 500: result_str = result_str[:500] + "..."
            
            updates_str = json.dumps(updates, default=str, ensure_ascii=False)
            
            system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“å®¶ç³»ç»Ÿçš„æ‰§è¡Œè®°å½•å‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€æ®µç®€æ´ã€å®¢è§‚çš„ä¸­æ–‡æ‰§è¡Œå°ç»“ã€‚

ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘
- æ­¥éª¤åç§°: {step_name}
- å·¥å…·: {tool_name}
- è¾“å…¥: {inputs_str}
- è¾“å‡º: {result_str}
- çŠ¶æ€æ›´æ–°: {updates_str}

ã€æ’°å†™è¦æ±‚ã€‘
1. **æå…¶ç®€æ´**ï¼šå­—æ•°æ§åˆ¶åœ¨ 80 å­—ä»¥å†…ã€‚
2. **å®¢è§‚é™ˆè¿°**ï¼šç›´æ¥é™ˆè¿°äº‹å®ï¼Œä¸è¦ä½¿ç”¨â€œæˆ‘â€ã€â€œç³»ç»Ÿâ€ã€â€œæ‰§è¡Œäº†â€ç­‰ä¸»è¯­ã€‚
3. **é‡ç‚¹çªå‡º**ï¼šæ ¸å¿ƒå…³æ³¨â€œæ ¹æ®ä»€ä¹ˆè¾“å…¥ï¼ˆå¦‚æ¡ä»¶ã€å…¬å¼ï¼‰ï¼Œå¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼ˆå…³é”®æ•°å€¼ï¼‰â€ã€‚
4. **é”™è¯¯å¤„ç†**ï¼šå¦‚æœè¾“å‡ºåŒ…å« errorï¼Œå¿…é¡»æ˜ç¡®æŒ‡å‡ºé”™è¯¯åŸå› ã€‚
5. **æ ¼å¼ç¤ºä¾‹**ï¼š
   - æŸ¥è¡¨ï¼ˆè¡¨Aï¼‰ï¼Œåœ¨æ¡ä»¶ x=1 ä¸‹è·å–åˆ° y=2ã€‚
   - æ ¹æ®å…¬å¼ a+b è®¡ç®—å¾—åˆ° c=3ã€‚
   - ç”¨æˆ·è¾“å…¥å˜é‡ dï¼Œå€¼ä¸º 4ã€‚
"""
            messages = [{"role": "system", "content": system_prompt.strip()}]
            
            response = self.llm_client.chat(messages, mode=self.mode, config_name=self.config_name)
            return response.strip()
            
        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            return f"æ‰§è¡Œå·¥å…· {tool_name} å®Œæˆã€‚æ›´æ–°å˜é‡: {list(updates.keys())}"
