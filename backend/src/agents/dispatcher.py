import time
import json
from typing import Dict, Any, Tuple, List
from src.core.contextStruct import SOP, Step
from src.core.memory import Memory, StepRecord
from src.tools.base import ToolRegistry
from src.core.llm import llm_client

class Dispatcher:
    def __init__(self):
        self.memory = Memory()
        
    def run(self, sop: SOP, initial_context: Dict[str, Any]):
        """
        Execute the SOP with the given initial context.
        """
        print(f"[{sop.id}] Starting execution: {sop.description}")
        self.memory.update_context(initial_context)
        
        # Simple linear execution for now
        # In a real FSM, we would follow next_step_id
        for step in sop.steps:
            self._execute_step(step)
            
        print(f"[{sop.id}] Execution finished.")
        return self.memory.global_context
        
    def _execute_step(self, step: Step):
        print(f"  -> Executing Step: {step.name or step.id} ({step.tool})")
        
        # [Hybrid Architecture Check]
        # If this step was generated by LLM analysis (Scenario A)
        if getattr(step, "analysis_status", None) == "analyzed":
            self._execute_analyzed_step(step)
            return

        # [Classic Logic] (Scenario B)
        # 1. Resolve Inputs
        tool_inputs = {}
        for key, value in step.inputs.items():
            resolved_value = self.memory.resolve_value(value)
            tool_inputs[key] = resolved_value
            
        # 2. Determine Tool (Static or Auto)
        target_tool_name = step.tool
        if target_tool_name == "auto":
            print("    [Auto] Detecting tool via LLM...")
            detected_tool, detected_inputs = self._smart_select_tool(step, tool_inputs)
            if detected_tool:
                print(f"    [Auto] Selected tool: {detected_tool}")
                target_tool_name = detected_tool
                # Merge detected inputs, preferring detected ones if they exist
                tool_inputs.update(detected_inputs)
            else:
                print("    [Auto] Failed to select tool.")
                self._record_step(step, tool_inputs, None, error="Auto-selection failed")
                return

        # 3. Get Tool
        tool = ToolRegistry.get_tool(target_tool_name)
        if not tool:
            error_msg = f"Tool '{target_tool_name}' not found"
            print(f"    Error: {error_msg}")
            self._record_step(step, tool_inputs, None, error=error_msg)
            return
            
        # 4. Execute Tool
        try:
            # Here we could insert an LLM call if inputs need formatting
            # But for now, trust the resolution
            result = tool.run(**tool_inputs)
            print(f"    Result: {result}")
            
            # 5. Process Outputs
            self._process_outputs(step, result)
            
            # 6. Record History
            self._record_step(step, tool_inputs, result)
            
        except Exception as e:
            print(f"    Error executing tool: {e}")
            self._record_step(step, tool_inputs, None, error=str(e))

    def _execute_analyzed_step(self, step: Step):
        """
        Execute a step that was analyzed by LLM (Hybrid Mode).
        Logic:
        1. Check if required inputs exist in context.
        2. Check if 'notes' exist.
        3. If (Missing Inputs OR Notes OR Tool='auto'), wake up LLM.
        4. Else, map inputs and execute directly.
        """
        context = self.memory.global_context
        missing_params = []
        ready_inputs = {}
        
        # 1. Check Inputs
        # step.inputs is Dict[param_name, description]
        for param_name, desc in step.inputs.items():
            if param_name in context:
                ready_inputs[param_name] = context[param_name]
            else:
                missing_params.append(f"{param_name} ({desc})")
                
        # 2. Decision Logic
        needs_llm = False
        reason = ""
        
        if missing_params:
            needs_llm = True
            reason = f"Missing parameters: {missing_params}"
        elif step.notes:
            needs_llm = True
            reason = f"Notes present: {step.notes}"
        elif step.tool == "auto":
            needs_llm = True
            reason = "Tool is 'auto'"
            
        if needs_llm:
            print(f"    [Hybrid] Waking up LLM... Reason: {reason}")
            self._smart_step_execution(step, reason, missing_params)
        else:
            print("    [Hybrid] Rule-based execution (All params ready, no notes).")
            # Execute directly
            self._execute_tool_safe(step.tool, ready_inputs, step)

    def _execute_tool_safe(self, tool_name: str, inputs: Dict[str, Any], step: Step):
        """Helper to execute tool and record history"""
        tool = ToolRegistry.get_tool(tool_name)
        if not tool:
            print(f"    Error: Tool {tool_name} not found")
            return
            
        try:
            result = tool.run(**inputs)
            print(f"    Result: {result}")
            # Update context with all result keys if it's a dict
            if isinstance(result, dict):
                self.memory.update_context(result)
            elif isinstance(result, str):
                # Try to guess key or store as 'last_result'
                self.memory.update_context({"last_result": result})
                
            self._record_step(step, inputs, result)
        except Exception as e:
            print(f"    Error: {e}")
            self._record_step(step, inputs, None, error=str(e))

    def _smart_step_execution(self, step: Step, reason: str, missing_params: List[str]):
        """
        LLM handles the execution:
        - Can ask user for input
        - Can search knowledge
        - Can execute the target tool
        """
        # Construct Prompt
        context_str = json.dumps(self.memory.global_context, default=str, ensure_ascii=False)
        if len(context_str) > 3000:
            context_str = context_str[:3000] + "..."
            
        system_prompt = f"""
You are the Step Executor of an expert system. You are facing a step that requires your attention.

Current Step:
- Name: {step.name}
- Description: {step.description}
- Notes/Warnings: {step.notes}
- Required Inputs: {step.inputs}

Context Variables:
{context_str}

Situation: {reason}

Your Goal: Complete this step or make progress towards it.

Available Actions (Output JSON):
1. ASK_USER: If parameters are missing and you cannot deduce them.
   {{ "action": "ask_user", "question": "..." }}
   
2. SEARCH_KNOWLEDGE: If you need to check textual regulations.
    {{ "action": "search_knowledge", "query": "..." }}
    
 3. TABLE_LOOKUP: If you need to find a value in a standard table (e.g. Dimensions of 10000 DWT ship).
    {{ "action": "table_lookup", "table_name": "...", "conditions": "...", "target_column": "..." }}

 4. EXECUTE_TOOL: If you have enough info to run the tool (calculator, etc).
    {{ "action": "execute_tool", "tool": "{step.tool if step.tool != 'auto' else 'appropriate_tool'}", "inputs": {{ ... }} }}
    
 5. SKIP: If this step is already done or irrelevant.
    {{ "action": "skip", "reason": "..." }}

Output ONLY the JSON.
"""
        messages = [{"role": "system", "content": system_prompt}]
        
        response = llm_client.chat(messages)
        
        # Parse JSON
        try:
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            
            action_data = json.loads(response.strip())
            action = action_data.get("action")
            
            print(f"    [AI Decision] {action}")
            
            if action == "ask_user":
                question = action_data.get("question")
                print(f"    >>> Asking User: {question}")
                # In a real system, we would pause here. 
                # For this prototype, we simulate user input or just log it.
                # Let's try to simulate 'user_input' tool execution
                user_tool = ToolRegistry.get_tool("user_input")
                if user_tool:
                    # In a real CLI, this would prompt input()
                    # For now, let's assume we can't block easily without IO redirection
                    # We will just print it.
                    pass
                    
            elif action == "search_knowledge":
                query = action_data.get("query")
                print(f"    >>> Searching Knowledge: {query}")
                # Call KnowledgeSearchTool
                search_tool = ToolRegistry.get_tool("knowledge_search")
                if search_tool:
                    search_result = search_tool.run(query=query)
                    print(f"    [Knowledge Found] {str(search_result)[:100]}...")
                    # Store result in context so next iteration (or re-try) can use it
                    self.memory.update_context({"knowledge_context": search_result})
                    # Recursively try to execute the step again with new knowledge?
                    # Or just let the loop continue?
                    # For now, let's just record it.
                else:
                    print("    [Error] knowledge_search tool not found.")
                
            elif action == "table_lookup":
                table_name = action_data.get("table_name")
                conditions = action_data.get("conditions")
                target_col = action_data.get("target_column")
                print(f"    >>> Looking up Table: {table_name}, Cond: {conditions}")
                
                lookup_tool = ToolRegistry.get_tool("table_lookup")
                if lookup_tool:
                    result = lookup_tool.run(table_name=table_name, query_conditions=conditions, target_column=target_col)
                    print(f"    [Table Result] {str(result)[:100]}...")
                    self.memory.update_context(result if isinstance(result, dict) else {"lookup_result": result})
                else:
                    print("    [Error] table_lookup tool not found.")

            elif action == "execute_tool":
                tool_name = action_data.get("tool")
                inputs = action_data.get("inputs", {})
                self._execute_tool_safe(tool_name, inputs, step)
                
            elif action == "skip":
                print(f"    Skipping step: {action_data.get('reason')}")
                
        except Exception as e:
            print(f"    [SmartExec Error] {e}")



    def _smart_select_tool(self, step: Step, current_inputs: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        """
        Use LLM to select the best tool and formulate inputs when step.tool is 'auto'.
        """
        tools_desc = ToolRegistry.list_tools()
        tools_str = "\n".join([f"- {name}: {desc}" for name, desc in tools_desc.items()])
        
        # Prepare context snapshot (truncated to avoid huge prompt)
        context_str = json.dumps(self.memory.global_context, default=str, ensure_ascii=False)
        if len(context_str) > 2000:
            context_str = context_str[:2000] + "...(truncated)"
            
        system_prompt = f"""
You are an intelligent agent dispatcher. Your task is to select the most appropriate tool to execute the current step.

Available Tools:
{tools_str}

Current Step Information:
- ID: {step.id}
- Description: {step.description_zh or step.description}
- Pre-resolved Inputs: {json.dumps(current_inputs, default=str, ensure_ascii=False)}

Global Context:
{context_str}

Instructions:
1. Analyze the step description and context.
2. Select the best tool from the available list to accomplish the step goal.
3. Extract or formulate the necessary arguments for the tool based on context and inputs.
4. Return a JSON object with "tool" and "inputs".

Example Output:
{{
  "tool": "calculator",
  "inputs": {{ "expression": "12 * 50" }}
}}
"""
        messages = [{"role": "system", "content": system_prompt}]
        try:
            response = llm_client.chat(messages)
            # clean response
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            
            data = json.loads(response.strip())
            return data.get("tool"), data.get("inputs", {})
        except Exception as e:
            print(f"Smart Selection Failed: {e}")
            return None, {}

            
    def _process_outputs(self, step: Step, result: Any):
        # Update global context based on output mapping
        if not step.outputs:
            return
            
        # If outputs is "*" map everything (if result is dict)
        if step.outputs == "*":
            if isinstance(result, dict):
                self.memory.update_context(result)
            else:
                self.memory.update_context({"last_result": result})
            return
            
        for context_key, result_path in step.outputs.items():
            # Simple extraction
            # If result_path is empty string, ".", or "result", use the whole result
            if not result_path or result_path == "." or result_path == "result":
                val = result
            elif isinstance(result, dict) and result_path in result:
                val = result[result_path]
            else:
                val = None # Or keep existing?
                
            if val is not None:
                self.memory.update_context({context_key: val})
                
    def _record_step(self, step: Step, inputs: Any, outputs: Any, error: str = None):
        record = StepRecord(
            step_id=step.id,
            tool_name=step.tool,
            inputs=inputs,
            outputs=outputs,
            status="failed" if error else "success",
            error=error
        )
        self.memory.add_history(record)
