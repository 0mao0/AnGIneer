import time
import json
import os
from typing import Dict, Any, Tuple, List
from src.core.contextStruct import SOP, Step
from src.core.memory import Memory, StepRecord
from src.tools.BaseTool import ToolRegistry
from src.core.llm import llm_client

class Dispatcher:
    def __init__(self, config_name: str = None, mode: str = "instruct", result_md_path: str = None):
        """åˆå§‹åŒ–æ‰§è¡Œå™¨ä¸Šä¸‹æ–‡ä¸æ¨¡å‹é…ç½®ã€‚"""
        self.memory = Memory()
        self.config_name = config_name
        self.mode = mode or "instruct"
        self.result_md_path = result_md_path
        self.variable_metadata = {}
        
        if self.result_md_path:
            # Initialize Markdown file
            with open(self.result_md_path, "w", encoding="utf-8") as f:
                f.write("# SOP æ‰§è¡Œæ—¥å¿— (LLM é£æ ¼å°ç»“ç‰ˆ)\n\n")
                f.write("> **è¯´æ˜**: æœ¬æ—¥å¿—å±•ç¤ºäº†æ¯ä¸€æ­¥çš„æ‰§è¡Œå°ç»“ä¸ Blackboard çŠ¶æ€å¿«ç…§ã€‚æ›´æ–°çš„å†…å®¹å·²é«˜äº®æ˜¾ç¤ºã€‚\n\n---\n\n")
        
    def run(self, sop: SOP, initial_context: Dict[str, Any]):
        """
        Execute the SOP with the given initial context.
        """
        print(f"[{sop.id}] Starting execution: {sop.description}")
        self.memory.update_context(initial_context)
        
        # Simple linear execution for now
        # In a real FSM, we would follow next_step_id
        for step in sop.steps:
            self._execute_step(step)
            
        print(f"[{sop.id}] Execution finished.")
        return self.memory.blackboard
        
    def _execute_step(self, step: Step):
        print(f"  -> Executing Step: {step.name or step.id} ({step.tool})")
        
        # [Hybrid Architecture Check]
        # If this step was generated by LLM analysis (Scenario A)
        if getattr(step, "analysis_status", None) == "analyzed":
            self._execute_analyzed_step(step)
            return

        # [Classic Logic] (Scenario B)
        # 1. Resolve Inputs
        tool_inputs = {}
        for key, value in step.inputs.items():
            resolved_value = self.memory.resolve_value(value)
            tool_inputs[key] = resolved_value
            
        # 2. Determine Tool (Static or Auto)
        target_tool_name = step.tool
        if target_tool_name == "auto":
            print("    [Auto] Detecting tool via LLM...")
            detected_tool, detected_inputs = self._smart_select_tool(step, tool_inputs)
            if detected_tool:
                print(f"    [Auto] Selected tool: {detected_tool}")
                target_tool_name = detected_tool
                # Merge detected inputs, preferring detected ones if they exist
                tool_inputs.update(detected_inputs)
            else:
                print("    [Auto] Failed to select tool.")
                self._record_step(step, tool_inputs, None, error="Auto-selection failed")
                return

        # 3. Execute Tool
        self._execute_tool_safe(target_tool_name, tool_inputs, step)

    def _execute_analyzed_step(self, step: Step):
        """
        Execute a step that was analyzed by LLM (Hybrid Mode).
        Logic:
        1. Resolve all inputs from context.
        2. Check if any resolved input indicates missing data (e.g. explicit None or unresolved vars).
        3. Check if 'notes' exist.
        4. If (Missing Inputs OR Notes OR Tool='auto'), wake up LLM.
        5. Else, execute directly.
        """
        context = self.memory.blackboard
        # Check if step outputs already exist in context
        if self._should_skip_step(step, context):
            self._record_step(step, {}, {"skipped": True, "reason": "value exists in context"})
            return
            
        missing_params = []
        ready_inputs = {}
        
        # 1. Resolve Inputs
        for param_name, value_expr in step.inputs.items():
            resolved_value = self.memory.resolve_value(value_expr)
            ready_inputs[param_name] = resolved_value
            
            # Simple check: if resolved value looks like an unresolved template or None
            if resolved_value is None:
                 missing_params.append(f"{param_name} (value is None)")
            elif isinstance(resolved_value, str) and "${" in resolved_value:
                 # Check if it's an unresolved reference
                 # This is a heuristic, but often useful
                 missing_params.append(f"{param_name} (unresolved: {resolved_value})")

        # 2. Decision Logic
        needs_llm = False
        reason = ""
        
        if missing_params:
            needs_llm = True
            reason = f"Missing parameters: {missing_params}"
        elif step.notes:
            needs_llm = True
            reason = f"Notes present: {step.notes}"
        elif step.tool == "auto":
            needs_llm = True
            reason = "Tool is 'auto'"
            
        if needs_llm:
            print(f"    [Hybrid] Waking up LLM... Reason: {reason}")
            # Pass resolved inputs to help LLM? 
            # Actually _smart_step_execution re-evaluates everything. 
            # But we should probably pass the missing params info.
            self._smart_step_execution(step, reason, missing_params)
        else:
            print("    [Hybrid] Rule-based execution (All params ready, no notes).")
            # Execute directly
            self._execute_tool_safe(step.tool, ready_inputs, step)

    def _should_skip_step(self, step: Step, context: Dict[str, Any]) -> bool:
        """Check if step outputs are already present in context."""
        if not step.outputs:
            return False
        # If outputs is "*" we can't easily check, so don't skip
        if step.outputs == "*":
            return False
            
        output_keys = list(step.outputs.keys())
        if not output_keys:
            return False
            
        # Check if all output keys exist and are not None
        # Exception: if the value in context is explicitly "None" string or something? No.
        return all(key in context and context.get(key) is not None for key in output_keys)

    def _execute_tool_safe(self, tool_name: str, inputs: Dict[str, Any], step: Step):
        """Helper to execute tool and record history"""
        tool = ToolRegistry.get_tool(tool_name)
        if not tool:
            print(f"    Error: Tool {tool_name} not found")
            return
            
        try:
            run_kwargs = dict(inputs)
            if self.config_name:
                run_kwargs["config_name"] = self.config_name
            if self.mode:
                run_kwargs["mode"] = self.mode
            result = tool.run(**run_kwargs)
            print(f"    Result: {result}")
            
            # Process outputs using the standard method
            updates = self._process_outputs(step, result)
            
            # Record history
            self._record_step(step, inputs, result)
            
            # Write log
            if self.result_md_path:
                self._write_markdown_log(step, inputs, result, updates)
                
        except Exception as e:
            print(f"    Error: {e}")
            self._record_step(step, inputs, None, error=str(e))
            if self.result_md_path:
                 self._write_markdown_log(step, inputs, {"error": str(e)}, {})

    def _smart_step_execution(self, step: Step, reason: str, missing_params: List[str]):
        """
        LLM handles the execution:
        - Can ask user for input
        - Can search knowledge
        - Can execute the target tool
        """
        # Construct Prompt
        context_str = json.dumps(self.memory.get_context_snapshot(), default=str, ensure_ascii=False)
        if len(context_str) > 3000:
            context_str = context_str[:3000] + "..."
            
        system_prompt = f"""
You are the Step Executor of an expert system. You are facing a step that requires your attention.

Current Step:
- Name: {step.name}
- Description: {step.description}
- Notes/Warnings: {step.notes}
- Required Inputs: {step.inputs}

Context Variables:
{context_str}

Situation: {reason}

Your Goal: Complete this step or make progress towards it.

Available Actions (Output JSON):
1. ASK_USER: If parameters are missing and you cannot deduce them.
   {{ "action": "ask_user", "question": "..." }}
   
2. SEARCH_KNOWLEDGE: If you need to check textual regulations.
    {{ "action": "search_knowledge", "query": "..." }}
    
 3. TABLE_LOOKUP: If you need to find a value in a standard table (e.g. Dimensions of 10000 DWT ship).
    {{ "action": "table_lookup", "table_name": "...", "conditions": "...", "target_column": "..." }}

 4. EXECUTE_TOOL: If you have enough info to run the tool (calculator, etc).
    {{ "action": "execute_tool", "tool": "{step.tool if step.tool != 'auto' else 'appropriate_tool'}", "inputs": {{ ... }} }}
    
 5. RETURN_VALUE: If the step is just setting a value or you know the answer directly.
    {{ "action": "return_value", "value": 0.15 }}

 6. SKIP: If this step is already done or irrelevant.
    {{ "action": "skip", "reason": "..." }}

Output ONLY the JSON.
"""
        messages = [{"role": "system", "content": system_prompt}]
        
        response = llm_client.chat(messages, mode=self.mode, config_name=self.config_name)
        
        # Parse JSON
        try:
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            
            action_data = json.loads(response.strip())
            action = action_data.get("action")
            
            print(f"    [AI Decision] {action}")
            
            if action == "return_value":
                value = action_data.get("value")
                print(f"    >>> Returning Value: {value}")
                # Process outputs directly
                updates = self._process_outputs(step, value)
                self._record_step(step, {}, value)
                if self.result_md_path:
                    self._write_markdown_log(step, {}, value, updates)

            elif action == "ask_user":
                question = action_data.get("question")
                print(f"    >>> Asking User: {question}")
                # Simulate user_input tool execution
                self._execute_tool_safe("user_input", {"question": question}, step)
                    
            elif action == "search_knowledge":
                query = action_data.get("query")
                print(f"    >>> Searching Knowledge: {query}")
                self._execute_tool_safe("knowledge_search", {"query": query}, step)
                
            elif action == "table_lookup":
                table_name = action_data.get("table_name")
                conditions = action_data.get("conditions")
                target_col = action_data.get("target_column")
                print(f"    >>> Looking up Table: {table_name}, Cond: {conditions}")
                
                inputs = {
                    "table_name": table_name,
                    "query_conditions": conditions,
                    "target_column": target_col
                }
                self._execute_tool_safe("table_lookup", inputs, step)

            elif action == "execute_tool":
                tool_name = action_data.get("tool")
                inputs = action_data.get("inputs", {})
                self._execute_tool_safe(tool_name, inputs, step)
                
            elif action == "skip":
                print(f"    Skipping step: {action_data.get('reason')}")
                
        except Exception as e:
            print(f"    [SmartExec Error] {e}")



    def _generate_step_summary(self, step_name: str, tool_name: str, resolved_inputs: Any, result: Any, updates: Dict[str, Any]) -> str:
        """Use LLM to generate a natural language summary of the step execution."""
        try:
            # Prepare data for prompt, truncating large structures
            inputs_str = json.dumps(resolved_inputs, default=str, ensure_ascii=False)
            if len(inputs_str) > 500: inputs_str = inputs_str[:500] + "..."
            
            result_str = json.dumps(result, default=str, ensure_ascii=False)
            if len(result_str) > 500: result_str = result_str[:500] + "..."
            
            updates_str = json.dumps(updates, default=str, ensure_ascii=False)
            
            system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“å®¶ç³»ç»Ÿçš„æ‰§è¡Œè®°å½•å‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€æ®µç®€æ´ã€å®¢è§‚çš„ä¸­æ–‡æ‰§è¡Œå°ç»“ã€‚

ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘
- æ­¥éª¤åç§°: {step_name}
- å·¥å…·: {tool_name}
- è¾“å…¥: {inputs_str}
- è¾“å‡º: {result_str}
- çŠ¶æ€æ›´æ–°: {updates_str}

ã€æ’°å†™è¦æ±‚ã€‘
1. **æå…¶ç®€æ´**ï¼šå­—æ•°æ§åˆ¶åœ¨ 80 å­—ä»¥å†…ã€‚
2. **å®¢è§‚é™ˆè¿°**ï¼šç›´æ¥é™ˆè¿°äº‹å®ï¼Œä¸è¦ä½¿ç”¨â€œæˆ‘â€ã€â€œç³»ç»Ÿâ€ã€â€œæ‰§è¡Œäº†â€ç­‰ä¸»è¯­ã€‚
3. **é‡ç‚¹çªå‡º**ï¼šæ ¸å¿ƒå…³æ³¨â€œæ ¹æ®ä»€ä¹ˆè¾“å…¥ï¼ˆå¦‚æ¡ä»¶ã€å…¬å¼ï¼‰ï¼Œå¾—åˆ°äº†ä»€ä¹ˆç»“æœï¼ˆå…³é”®æ•°å€¼ï¼‰â€ã€‚
4. **é”™è¯¯å¤„ç†**ï¼šå¦‚æœè¾“å‡ºåŒ…å« errorï¼Œå¿…é¡»æ˜ç¡®æŒ‡å‡ºé”™è¯¯åŸå› ã€‚
5. **æ ¼å¼ç¤ºä¾‹**ï¼š
   - æŸ¥è¡¨ï¼ˆè¡¨Aï¼‰ï¼Œåœ¨æ¡ä»¶ x=1 ä¸‹è·å–åˆ° y=2ã€‚
   - æ ¹æ®å…¬å¼ a+b è®¡ç®—å¾—åˆ° c=3ã€‚
   - ç”¨æˆ·è¾“å…¥å˜é‡ dï¼Œå€¼ä¸º 4ã€‚
"""
            messages = [{"role": "system", "content": system_prompt.strip()}]
            
            response = llm_client.chat(messages, mode=self.mode, config_name=self.config_name)
            return response.strip()
            
        except Exception as e:
            print(f"Error generating summary: {e}")
            # Fallback to simple summary
            return f"æ‰§è¡Œå·¥å…· {tool_name} å®Œæˆã€‚æ›´æ–°å˜é‡: {list(updates.keys())}"

    def _write_markdown_log(self, step: Step, inputs: Any, result: Any, updates: Dict[str, Any]):
        """Write step execution details to Markdown file"""
        if not self.result_md_path:
            return
            
        blackboard_values = self.memory.blackboard
        step_id = step.id
        step_name = step.name or step_id
        tool_name = step.tool
        description = step.description or ""
        
        # Determine current step note based on tool
        current_step_note = f"å·¥å…·: {tool_name}"
        if tool_name == "table_lookup":
             table_name = inputs.get('table_name', '') if isinstance(inputs, dict) else ''
             current_step_note = f"æŸ¥è¡¨: {table_name}"
        elif tool_name == "calculator":
             expr = inputs.get('expression', '') if isinstance(inputs, dict) else ''
             if len(expr) > 25:
                 expr = expr[:22] + "..."
             current_step_note = f"å…¬å¼: {expr}" if expr else "å…¬å¼è®¡ç®—"
        elif tool_name == "user_input":
             current_step_note = "ç”¨æˆ·è¾“å…¥"
        elif tool_name == "auto":
             current_step_note = "è‡ªåŠ¨ç”Ÿæˆ"
             
        # Update metadata for new variables
        for key in updates:
            self.variable_metadata[key] = {
                "source_step": step_id,
                "note": current_step_note
            }
        
        with open(self.result_md_path, "a", encoding="utf-8") as f:
            f.write(f"## {step_id}: {step_name}\n\n")
            
            # 1. å†™å…¥ LLM å°ç»“
            llm_summary = self._generate_step_summary(step_name, tool_name, inputs, result, updates)
            f.write(f"**LLM å°ç»“**:{llm_summary}\n\n")
            
            # 2. å†™å…¥ Blackboard æ›´æ–°è¡¨æ ¼
            f.write(f"**Blackboard çŠ¶æ€**:\n\n")
            f.write("| åºå· | å‚æ•° | ç±»å‹ | å–å€¼ | çŠ¶æ€ | å¤‡æ³¨ |\n")
            f.write("| --- | --- | --- | --- | --- | --- |\n")
            
            # å›ºå®šé¡ºåºï¼šæŒ‰å­—æ¯åºæ’åº
            all_keys = sorted(blackboard_values.keys())
            
            for idx, key in enumerate(all_keys, 1):
                val = blackboard_values.get(key)
                
                # Default values
                status = "âšª å·²çŸ¥é‡"
                note = "-"
                
                if key in updates:
                    status = f"ğŸŸ¢ {step_id} ç»“æœ"
                    note = current_step_note
                elif key in self.variable_metadata:
                    meta = self.variable_metadata[key]
                    source = meta.get("source_step", "Unknown")
                    status = f"ğŸŸ¡ {source} æ±‚è§£"
                    note = meta.get("note", "-")
                else:
                    status = "âšª å·²çŸ¥é‡"
                    note = "åˆå§‹å‚æ•°"

                # Type Inference (Simple)
                val_type = type(val).__name__
                if isinstance(val, (int, float)):
                    val_type = "æ•°å€¼"
                elif isinstance(val, str):
                    val_type = "å­—ç¬¦ä¸²"
                
                # Format Value (Truncate if too long)
                val_str = str(val)
                # Escape pipe characters to avoid breaking the table
                val_str = val_str.replace("|", "\\|").replace("\n", " ")
                if len(val_str) > 50:
                    val_str = val_str[:47] + "..."

                f.write(f"| {idx} | {key} | {val_type} | {val_str} | {status} | {note} |\n")
                
            f.write("\n")
            
            # 3. è¯¦ç»†å·¥å…·æ—¥å¿—ï¼ˆæŠ˜å ï¼‰
            f.write("<details>\n<summary>ç‚¹å‡»æŸ¥çœ‹å·¥å…·è°ƒç”¨è¯¦æƒ…</summary>\n\n")
            f.write(f"**è¯´æ˜**: {description}\n\n")
            f.write(f"**å·¥å…·**: `{tool_name}`\n\n")
            f.write("**è¾“å…¥**:\n")
            f.write(f"```json\n{json.dumps(inputs, ensure_ascii=False, indent=2)}\n```\n\n")
            f.write("**è¾“å‡º**:\n")
            f.write(f"```json\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```\n\n")
            f.write("</details>\n\n")
            f.write("---\n\n")

    def _smart_select_tool(self, step: Step, current_inputs: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
        """
        Use LLM to select the best tool and formulate inputs when step.tool is 'auto'.
        """
        tools_desc = ToolRegistry.list_tools()
        tools_str = "\n".join([f"- {name}: {desc}" for name, desc in tools_desc.items()])
        
        # Prepare context snapshot (truncated to avoid huge prompt)
        context_str = json.dumps(self.memory.get_context_snapshot(), default=str, ensure_ascii=False)
        if len(context_str) > 2000:
            context_str = context_str[:2000] + "...(truncated)"
            
        system_prompt = f"""
You are an intelligent agent dispatcher. Your task is to select the most appropriate tool to execute the current step.

Available Tools:
{tools_str}

Current Step Information:
- ID: {step.id}
- Description: {step.description_zh or step.description}
- Pre-resolved Inputs: {json.dumps(current_inputs, default=str, ensure_ascii=False)}

Global Context:
{context_str}

Instructions:
1. Analyze the step description and context.
2. Select the best tool from the available list to accomplish the step goal.
3. Extract or formulate the necessary arguments for the tool based on context and inputs.
4. Return a JSON object with "tool" and "inputs".

Example Output:
{{
  "tool": "calculator",
  "inputs": {{ "expression": "12 * 50" }}
}}
"""
        messages = [{"role": "system", "content": system_prompt}]
        try:
            response = llm_client.chat(messages, mode=self.mode, config_name=self.config_name)
            # clean response
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0]
            elif "```" in response:
                response = response.split("```")[1].split("```")[0]
            
            data = json.loads(response.strip())
            return data.get("tool"), data.get("inputs", {})
        except Exception as e:
            print(f"Smart Selection Failed: {e}")
            return None, {}

            
    def _process_outputs(self, step: Step, result: Any) -> Dict[str, Any]:
        # Update global context based on output mapping
        updates = {}
        if not step.outputs:
            return updates
            
        # If outputs is "*" map everything (if result is dict)
        if step.outputs == "*":
            if isinstance(result, dict):
                updates = result
                self.memory.update_context(result)
            else:
                updates = {"last_result": result}
                self.memory.update_context(updates)
            return updates
            
        for context_key, result_path in step.outputs.items():
            # Simple extraction
            # If result_path is empty string or ".", use the whole result
            if not result_path or result_path == ".":
                val = result
            # If result_path is "result", extract the 'result' field from dict
            elif result_path == "result":
                if isinstance(result, dict) and "result" in result:
                    val = result["result"]
                else:
                    val = result  # Fallback to whole result if no 'result' field
            elif isinstance(result, dict) and result_path in result:
                val = result[result_path]
            else:
                # Try to treat result_path as a literal constant (e.g. "0.15", "-1", "True")
                try:
                    # Check for boolean first
                    if result_path.lower() == "true":
                         val = True
                    elif result_path.lower() == "false":
                         val = False
                    else:
                         # Try float/int
                         # Remove whitespace
                         rp = result_path.strip()
                         val = float(rp)
                         # Convert to int if it's an integer value and original string didn't look like a float (optional)
                         if val.is_integer() and '.' not in rp:
                             val = int(val)
                except:
                     val = None
                
            if val is not None:
                updates[context_key] = val
                self.memory.update_context({context_key: val})
        
        return updates
                
    def _record_step(self, step: Step, inputs: Any, outputs: Any, error: str = None):
        record = StepRecord(
            step_id=step.id,
            tool_name=step.tool,
            inputs=inputs,
            outputs=outputs,
            status="failed" if error else "success",
            error=error
        )
        self.memory.add_step_io({
            "step_id": step.id,
            "tool_name": step.tool,
            "inputs": inputs,
            "outputs": outputs,
            "status": "failed" if error else "success",
            "error": error
        })
        self.memory.add_history(record)
