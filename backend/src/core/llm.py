import os
import time
import json
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class LLMClient:
    """
    LLM å®¢æˆ·ç«¯ç±»ï¼Œè´Ÿè´£ç®¡ç†å¤šä¸ª LLM é…ç½®å¹¶å¤„ç†å¯¹è¯è¯·æ±‚ã€‚
    é»˜è®¤ä¼˜å…ˆä½¿ç”¨ NVIDIA æä¾›çš„æ¨¡å‹ï¼Œæ”¯æŒè‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨é…ç½®ã€‚
    """
    def __init__(self):
        # åŠ è½½æ‰€æœ‰ LLM é…ç½®ï¼ŒNVIDIA æ”¾åœ¨ç¬¬ä¸€ä½ä½œä¸ºé»˜è®¤
        self.configs = [
            {
                "name": "NVIDIA é…ç½®",
                "api_key": os.getenv("NVIDIA_API_KEY"),
                "base_url": os.getenv("NVIDIA_API_URL"),
                "model": os.getenv("NVIDIA_MODEL") # é»˜è®¤æ¨¡å‹
            },
            {
                "name": "ç§æœ‰é…ç½® (Private)",
                "api_key": os.getenv("Private_ALIYUN_API_KEY"),
                "base_url": os.getenv("Private_ALIYUN_API_URL"),
                "model": os.getenv("Private_ALIYUN_MODEL")
            },
            {
                "name": "å…¬å…±é…ç½® (Public)",
                "api_key": os.getenv("Public_ALIYUN_API_KEY"),
                "base_url": os.getenv("Public_ALIYUN_API_URL"),
                "model": os.getenv("Public_ALIYUN_MODEL")
            }
        ]
        
    def chat(self, messages: list, temperature: float = 0.1, model: str = None) -> str:
        """
        å‘é€å¯¹è¯è¯·æ±‚å¹¶è·å–å“åº”ã€‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: ç”Ÿæˆæ¸©åº¦
            model: å¯é€‰ï¼ŒæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ IDã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹ã€‚
            
        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„å›ç­”å†…å®¹
        """
        last_error = None
        
        for config in self.configs:
            if not config["api_key"] or not config["base_url"]:
                continue
            
            # ç¡®å®šå½“å‰ä½¿ç”¨çš„æ¨¡å‹
            current_model = model if model and config["name"] == "NVIDIA é…ç½®" else config["model"]
            
            # è®°å½•æ—¥å¿—ï¼šå¼€å§‹è¿æ¥
            print("\n" + "="*50)
            print(f"ğŸ¤– [LLM å‘¼å«] æ­£åœ¨è¿æ¥: {config['name']}")
            print(f"   æ¨¡å‹: {current_model}")
            print(f"   åœ°å€: {config['base_url']}")
            print("-" * 20)
            print("ğŸ“¤ [è¾“å…¥æ¶ˆæ¯]:")
            for msg in messages:
                role = msg.get('role', 'æœªçŸ¥')
                content = msg.get('content', '')
                print(f"   [{role.upper()}]: {content}")
            print("-" * 20)

            start_time = time.time()
            
            try:
                # å¦‚æœåœ°å€ä»¥ /chat/completions ç»“å°¾ï¼Œåˆ™æ¸…ç†æ‰ï¼ŒOpenAI å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨è¡¥å…¨
                base_url = config["base_url"]
                if base_url.endswith("/chat/completions"):
                    base_url = base_url.replace("/chat/completions", "")
                
                client = OpenAI(
                    api_key=config["api_key"],
                    base_url=base_url
                )
                
                response = client.chat.completions.create(
                    model=current_model,
                    messages=messages,
                    temperature=temperature
                )
                
                content = response.choices[0].message.content
                duration = time.time() - start_time
                
                # è®°å½•æ—¥å¿—ï¼šæˆåŠŸè¿”å›
                print(f"ğŸ“¥ [è¾“å‡ºå“åº”] (è€—æ—¶: {duration:.2f}ç§’):")
                print(f"   {content}")
                print("="*50 + "\n")
                
                return content
                
            except Exception as e:
                duration = time.time() - start_time
                print(f"âŒ [é”™è¯¯] (è€—æ—¶: {duration:.2f}ç§’): {str(e)}")
                print("="*50 + "\n")
                last_error = e
                continue
                
        # å¦‚æœæ‰€æœ‰é…ç½®éƒ½å°è¯•å¤±è´¥
        raise last_error or ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ LLM é…ç½®")
            
# å…¨å±€å•ä¾‹
llm_client = LLMClient()
